{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "INd8GhYikYZi"
      },
      "outputs": [],
      "source": [
        "#! Python3\n",
        "# useful libs\n",
        "import numpy as np\n",
        "import operator\n",
        "import re\n",
        "import collections\n",
        "import nltk\n",
        "from difflib import ndiff\n",
        "import seaborn as sns\n",
        "import numpy.linalg as la\n",
        "import six\n",
        "\n",
        "# Preprocessing\n",
        "from gensim.utils import lemmatize\n",
        "\n",
        "# vectorizers\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAm6KSLBkYZn"
      },
      "source": [
        "# (a) Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "64bwaAHXkYZr",
        "outputId": "a45be3d8-b296-4511-d0f6-576fb1d3e2de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor file_name in [\\'yelp_labelled\\', \\'amazon_cells_labelled\\', \\'imdb_labelled\\']:    \\n    for line in open(\\'./data/{}.txt\\'.format(file_name), \\'r\\', encoding=\\'utf-8\\'):\\n        st, sc = line.split(\"\\t\")\\n#         sentances.append(unicode(st, \"utf-8\"))\\n        sentances.append(st)\\n#         sentances.append(str(st.encode()))\\n\\n        scores.append(sc[0:-1])\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "sentances, scores = [], []\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1P8jSclCPfpFtTMdXzZy3D9DPTs9yOhb5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYR08vwK1gdE",
        "outputId": "5c62dbb0-f7a0-483c-abf9-9dc77047d4e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P8jSclCPfpFtTMdXzZy3D9DPTs9yOhb5\n",
            "To: /content/RTOnoLocWpol3.csv\n",
            "100% 41.6M/41.6M [00:01<00:00, 37.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"RTOnoLocWpol3.csv\")"
      ],
      "metadata": {
        "id": "GAZJIKmc1hgc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "#df = df[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "t24xbbzr1hj0",
        "outputId": "ee515dd1-bde3-45bf-9b32-5c859722aee2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>UserName</th>\n",
              "      <th>ScreenName</th>\n",
              "      <th>Location</th>\n",
              "      <th>TweetAt</th>\n",
              "      <th>OriginalTweet</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3798</td>\n",
              "      <td>48750</td>\n",
              "      <td>@Gotham2Face</td>\n",
              "      <td>2021-12-12 23:59:39+00:00</td>\n",
              "      <td>@Gotham2Face Talia waits for Harvey’s return, ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.087500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3799</td>\n",
              "      <td>48751</td>\n",
              "      <td>Genève, Suisse</td>\n",
              "      <td>2021-12-12 23:56:07+00:00</td>\n",
              "      <td>The Obamas Return to Chicago https://t.co/uqvJ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3800</td>\n",
              "      <td>48752</td>\n",
              "      <td>STATEWIDE BABY</td>\n",
              "      <td>2021-12-12 23:52:35+00:00</td>\n",
              "      <td>@mrosen23 @BrrrBailey @Puleo_Andrew @NYSPEF An...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.022487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3801</td>\n",
              "      <td>48753</td>\n",
              "      <td>30.399941,-97.705223</td>\n",
              "      <td>2021-12-12 23:47:33+00:00</td>\n",
              "      <td>Being a creator is high esteem and realistic. ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.108889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3802</td>\n",
              "      <td>48754</td>\n",
              "      <td>30.399941,-97.705223</td>\n",
              "      <td>2021-12-12 23:47:33+00:00</td>\n",
              "      <td>The biggest driving force for office returns s...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.107143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  UserName  ...  Sentiment  polarity\n",
              "0           0      3798  ...   positive  0.087500\n",
              "1           1      3799  ...    neutral  0.000000\n",
              "2           2      3800  ...   positive  0.022487\n",
              "3           3      3801  ...   positive  0.108889\n",
              "4           4      3802  ...   positive  0.107143\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "FoRAFilq5YfM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iItnuB4L3LJi",
        "outputId": "0aaaa795-771e-49dd-d6a9-1ee02a0f13f3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0       78\n",
              "UserName         78\n",
              "ScreenName       78\n",
              "Location         78\n",
              "TweetAt          78\n",
              "OriginalTweet    78\n",
              "Sentiment        78\n",
              "polarity         78\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.Sentiment =df.Sentiment.map(dict(positive=1, neutral=0 , negative=0))"
      ],
      "metadata": {
        "id": "V29LCMOv2-9F"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentances = list(df.OriginalTweet)"
      ],
      "metadata": {
        "id": "p8ezsY5T2IqW"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = list(df.Sentiment)"
      ],
      "metadata": {
        "id": "fjn0jS7U2RfJ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "t_r9vCAskYZu",
        "outputId": "a160a601-a504-48cf-98e4-4e036f97fcd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 41, 0: 37})\n"
          ]
        }
      ],
      "source": [
        "counter = collections.Counter(scores)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D42dTiYkYZx"
      },
      "source": [
        "### Explanation\n",
        "Yes the labels are balanced. \n",
        "By reading each line of the training txt files, we got lists of sentences and scores. \n",
        "Using collection.Counter, we get to know the number of each label in the score lists. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdJiRKiOkYZ0"
      },
      "source": [
        "# (b) Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkiSt_bE2iht",
        "outputId": "b421170f-e4df-47a9-d8e8-23bbbe1ce6f5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pattern in /usr/local/lib/python3.7/dist-packages (3.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.19.5)\n",
            "Requirement already satisfied: mysqlclient in /usr/local/lib/python3.7/dist-packages (from pattern) (2.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from pattern) (6.0.8)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from pattern) (1.0.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from pattern) (20211012)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from pattern) (18.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from pattern) (0.8.11)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (2.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (3.1.0)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (3.4.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.5.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.12.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (3.4.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->pattern) (4.1.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2018.9)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->pattern) (1.0.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->pattern) (3.6.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->pattern) (3.2.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.6.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (36.0.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "collapsed": true,
        "id": "MFAxsDPRkYZ2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub(\"@\",\"\", text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_processed = []\n",
        "for i in sentances :\n",
        "  sentences_processed.append(clean_text(i))"
      ],
      "metadata": {
        "id": "3fr4mRBl8Ie1"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3WKJkps8UCY",
        "outputId": "f83d9f7e-6478-4898-9c51-a440e70e3235"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' talia waits for harvey’s return, bruce getting up from bed and going to his office where he spent plenty of evenings holed away. it always made the manor feel larger, somehow. like she was kept out of the way one end, far away from everyone else even if it wasn’t entirely true. —',\n",
              " 'the obamas return to chicago  via obamafoundation',\n",
              " \" brrrbailey puleo_andrew nyspef anything less than filing suit to stop return to office with a #delta #omicron combined surge about to slam ny (generally  weeks behind uk) is a half-measure. our members are catching #covid left and right while hq appears to be doing nothing. we'd love to be proven wrong.\",\n",
              " 'being a creator is high esteem and realistic. % of gen z report being a freelancer, and, they are in a life place to swap our corporate life for freelance. gen z will limit the ability for execs to mandate a return to office.',\n",
              " 'the biggest driving force for office returns seems to be executives within certain demographics, but gen z is different.',\n",
              " 'you can continue to access information through bca’s phone system at any time on   .we will return your emails and phone calls once the office reopens in january, .',\n",
              " \"nickapoodle avengerresister if you had bothered to check what state i'm from, you would see why i loathe ted cruz more than lindsey graham. they're both sleazebags, but cruz is on of my senators. he claims to serve all texans, yet his office never returns letters or phone calls.\",\n",
              " 'professional me is out of the office until spring. this winter is single-mindedly dedicated to rest and healing. (i leave you with the below thread until my full return.)  ',\n",
              " ' hi i wanted to return an item for an exchange and i’ve already printed the shipping label but i wanted to know if can i send it back by putting it in a mail box or do i have to go directly to the post office to send it back?',\n",
              " \" i'm going to keep my head up; sat. army loss was not expected, today if you would have told the tweety birds would get by the cardiac cats i would have replied #keeppounding , and now i've got to return to the office to a water cooler where the trash talk will flow aplenty. yikes \",\n",
              " 'the end of a return-to-office date ',\n",
              " \"i know it's you. . .why won't you return to the sound of silence?(cymbal clash)🎶 restless streets you walk aloooonee! narrow streets of cobblestooonne! afton stood in my way, and he screamed as he gazed upon the words of the children that were written on the office halls! 🎶 \",\n",
              " 'the mirage of reaching a nuclear agreement with iran - the regime will continue to deceive the world while clandestinely working to reach the point of no return, writes amsafavi in thenatlinterest.#jcpoa #iran ',\n",
              " 'i don\\'t wanna assume every \"i want to return to the office\" post is an astroturfer but i recently caught one. the guy was spamming anti work from home articles on reddit lol',\n",
              " \"pat_blayne jusinsider dbseymour i don't agree - we'd good advice here regards people returning to nz &amp; further for an essential worker return completed within a week of the paper-work being filed &amp; they have since returnedbut then i only go to one office, how many of these offices do you frequent &amp; what for?\",\n",
              " \"microsofthelps any way to get my product key for a  hup purchase of office when entering the email i used to purchase the software doesn't return the key? i have the old email and order#, but msft doesn't seem to have the key under that email.\",\n",
              " 'and they always look so frustrated when the post office is closed. try the back entrance and it’s locked, go around the front, locked. return to car all pissed off. this is on you, bro.',\n",
              " 'fascinating culture data: in , all major tech companies embraced so-called \"empathy language.\" in , the most established tech companies abandoned it again (just as they started insisting everyone had to return to the office) ',\n",
              " 'end of a return-to-office date: we’ll get back to you, say\\xa0firms ',\n",
              " 'the roman street and marlow boys christmas returns to the mobile saenger theatre friday, december ! celebrate the season with christmas classics done in a new and exciting way! tickets on sale now at the box office or  ',\n",
              " 'the end of a return-to-office date ',\n",
              " 'jan was looking achievable for many employees to give up their sweatpants and return to the office. omicron had other plans, and companies are once again delaying that decision. office reopening plans seem more and more like wishful thinking.   ',\n",
              " 'since donald j. trump, jr. famously came down the escalator at trump tower to declare his candidacy for the office of the presidency, our government has feared him. no other potus has been systematically crucified the way he has been.yet, he wants to return.my kind of guy!👊🇺🇸',\n",
              " 'end of a return-to-office date: we’ll get back to you, say\\xa0f ',\n",
              " ' i’m retired from full time in the nhs, i just do the occasional shift so i’ve got no skin the game. i just remember working from home was supposed to be great and when johnson was saying people should return to the office people were disagreeing. not a tory btw.',\n",
              " '“…he tested negative for covid- upon his return to johannesburg (from his trip to west africa) on december , according to his office.” - cnn',\n",
              " 'imgrund many gta corps are asking their staff to return to office by jan/feb, will “ days a wk in office” work with the current situation?',\n",
              " 'companies rethink return-to-office plans amid omicron cases (from ap) ',\n",
              " 'drawplaydave i suppose this is a fair question at this point.   question: early returns are not great for zw, but... if mlf continues to grow &amp; is better than today &amp; they put him in positions to succeed, i think he will (eventually).  i do believe saleh is our coach for a long time',\n",
              " \"billcassidy is my representative. i'm asking him to please support voter rights bill so we can get trump out of politics. if he returns to office he won't leave. his supporters get their news from trump cronies and don't hear the truth w/ facts to back it up.\",\n",
              " 'me when i return back to the office ',\n",
              " 'the end of a return-to-office date ',\n",
              " 'end of a return-to-office date: we’ll get back to you, say\\xa0firms ',\n",
              " 'five things we’ve learnt in  that will help make next year better.number .\\xa0 the hybrid return to the office formula.there are just four things you need to know to figure out the right way to do hybrid - / who and / what, determines / where and…',\n",
              " 'companies finally admit they don\\'t know when they\\'re returning to offices:  google, apple, cnn, and ford have all postponed their \"return to office\" date, reports the new york times. (alternate url here.) the times also cites a gartner survey of  execu…',\n",
              " \"major us tech firms keep pushing back their return-to-office deadlines. maybe it's time to admit defeat.  #worktrends #futureofwork - talentculture meghanmbiro neilmilliken evankirstel avrohomg psb_dc  kkruse sarbjeetjohal facsdepa \",\n",
              " 'companies finally admit they don\\'t know when they\\'re returning to offices:  #slashdot google, apple, cnn, and ford have all postponed their \"return to office\" date, reports the new york times. (alternate url here.) the times also cites a gartner survey of…',\n",
              " 'the wem team will be taking some well-deserved time off over the christmas period this year. we will be out of office from the  december through to the  january. we are already excited to hit the ground running when we return to the office in ! ',\n",
              " 'remote work: sunday reads⚡ kpmg gives 👎 on long-term remote working⚡ remote work should be (mostly) async⚡ how to kill the american office for good⚡ return-to-office chaos is the best thing to happen to consultants since ',\n",
              " 'the end of a return-to-office date ',\n",
              " 'nestsaispaskut jessehawken mcu had only white male leads for over a decade lmao and considering box office returns lately will go back to that and shang chi sequels',\n",
              " 'google to give additional staff bonus this year as it postpones return-to-office plans ',\n",
              " 'the end of a return-to-office date  via politicalwire',\n",
              " 'zaichishka i got my partner to post a return back to australia and i told him just to write merchandise return on the customs, but he called me from the post office, oh the details they wanted, it was cray…',\n",
              " 'a return to the office goes hand in hand with conversations about #pay and #benefits.use existing knowledge of the company and your last offer as leverage, or #negotiate better #flexibility.#boomerang #talent #careeradvice (yahoofinance) ',\n",
              " \"the return-to-office dilemma is the best thing to happen to consultants since , bloomberg's matthew boyle reports. \",\n",
              " 'the end of a return-to-office date  #pandemic # #economy #coporations #labourmarket #workers #working #wfh #',\n",
              " '_jamesgtfo star wars return of the jedi would not be considered a box office success to the studio if it did not turn a profit in its theatrical run.you know where star wars gets its money? in merchandise. james, there is so much nuance here that you are unaware of.',\n",
              " \"this is encore's final week in office for . now's the time to get in touch with us to discuss any requirements for your upcoming events before we return on january , ! #encoreapac #happyholidays #christmasshutdown \",\n",
              " 'how do you transition workers back to the office, in a way that feels good &amp; safe to them? you carefully choreograph the return-to-office #employeeexperience, like molsoncoors per this wsj piece from maloneyfiles. #cx #custexp #hr #employeeengagement',\n",
              " '_jamesgtfo \"according to lucasfilm, return of the jedi, despite having earned $ million at the box office against a budget of $. million, \"has never gone into profit\".\" -- house of gucci is fuuuuucked',\n",
              " 'call () - demand the senate pass the protecting our democracy act (poda)  poda is our chance to prevent future presidential corruption by making presidential candidates release  years of tax returns, preventing presidents from profiting from office, and ensuring',\n",
              " 'for the -year-old second-generation governor who took office just weeks before the commonwealth’s first covid- cases hit in , the weekend marked a sudden return to emergency footing. ',\n",
              " 'for many companies, the return-to-office date is gone. it’s been replaced with “we’ll get back to you.” ',\n",
              " 'addressing return-to-office woes for a safer work\\xa0life ',\n",
              " 'as rent the runway continues to look for profitability, manager ashley rocha-rinere says, \"rental retailers’ problems are likely to continue into the fourth quarter as a return-to-office will likely be delayed again.\" read more: ',\n",
              " \"do we have to return to office? why company hybrid plans are more 'shybrid' - bloomberg \",\n",
              " \" just like the invisible patient in the dr's office - doc tells nurse 'tell him i can't see him today' ;p\",\n",
              " \"end of a return-to-office date: we'll get back to you, say firms some workers have returned to their cubicles in recent months, with office occupancy across the united states rising from % in august to % this month #latest news by #businessstandard \",\n",
              " 'sdut lived here all my life &amp; see sports venues shrinking as city grows. never a professional team to return because of size. no super bowl but they sure cater to homeless instead.  bad govt.  need new office holders.',\n",
              " '“return-to-office dates used to be like talismans; the chief executives who set them seemed to wield some power over the shape of the months to come. then the dates were postponed, and postponed again. at some point the spell was broken.” ',\n",
              " 'the #omicron variant is prompting return-to-office delays and reassessments. ⚕ neuroleadership  thehartford kearney cozen_oconnor ',\n",
              " 'gifttanz we are, we have just changed location to an office in newtongrange, but can still offer a collect and return for customers',\n",
              " 'google and ford are among those once again delaying their return-to-office plans, while other businesses whose workers are already back to the office are considering adding extra precautions like masks and covid- testing.  ',\n",
              " 'the end of a return-to-office date - the new york times ',\n",
              " 'bob_shrewsbury nicktolhurst early canvassing returns and the fact we had the activist base to get  leaflets out on weekend  showed it had real potential. labour didn’t even open an office and zero front benchers have campaigned',\n",
              " 'companies rethink return-to-office plans amid omicron cases - the atlanta journal constitution ',\n",
              " \"faultymoonpie unreachable.. to be unreachable by those we want to avoidyrs ago i read  about top hollywood exec. his wife vetoed answerphone  home . she didn't want th endless invites, requests, burden  return call\",\n",
              " \"major us tech firms keep pushing back their return-to-office deadlines. maybe it's time to admit defeat. \",\n",
              " 'the end of a return-to-office date ',\n",
              " 'tinyelvisbkwd someone used my address for illegal behavior. write return to sender in big red letters on it. put the flag up on your mailbox. let the post office deal with it. it’s their job.',\n",
              " 'the end of a return-to-office date via nytimes #emergingtech',\n",
              " 'for many, the return to office will never be full-time and the perpetually delayed mirage of a return to the office environment of  only makes the hybrid/permawfh future more likely. fyi jonerp ',\n",
              " 'realities—————the end of a return-to-office date ',\n",
              " ' oh, it\\'s a necessity! why didn\\'t you say so! necessities should be completely free!that \"markup\" covers- doctors, nurses, janitors, lawyers and office staff to navigate the endless minefield of government and insurance regulations and, hopefully, produce a return on investment.',\n",
              " \"you can still circle back and touch base — but the vernacular of work life for many has changed just as much as work has.whether you're preparing to return to the office, or settling in for a long remote winter, here are some words you may find handy. \",\n",
              " \"hey, are there any dsa or dsa-endorsed candidates running for office in  in tennessee?if so, reply here and/or send me a message. i'm thinking about doing a thing.all offices are welcome, from us rep to state rep to local office.\",\n",
              " 'idk how them finna return me to the office but my titties will no longer be subjected to that kinda torture idc idc']"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99HcnZhhkYZ4"
      },
      "source": [
        "### Preprocessing explanations\n",
        "1) We should lowercase all of the words, because capitalized letters with make same words be treated as different ones.\n",
        "\n",
        "2) We should strip punctuations because they do not contribute to sentiment.\n",
        "\n",
        "3) Stop words are the most commonly occuring words which are not relevant in the context of the data and do not contribute any deeper meaning to the phrase. In this case contain no sentiment.\n",
        "\n",
        "4) We should do lemmatization.This process finds the base or dictionary form of the word known as the lemma. This is done through the use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations). This normalization is similar to stemming but takes into account the context of the word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep9zKdLPkYZ5"
      },
      "source": [
        "# (c) Split Training and Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "collapsed": true,
        "id": "oyXYccPZkYZ7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(sentances, scores, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "4cjrwtqk42JJ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yt6iOrzkYZ-"
      },
      "source": [
        "## (d) Bag of Words\n",
        "Why should we vectorize training set first and then go through testing set?<br/>\n",
        "1) Here we should vectorize the training set standalone because testing set could contain words that are not contained in training set. <br/>\n",
        "2) We will vectorize testing set based on the feature vector generated by training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "UvapCZfakYaA",
        "outputId": "418b1edf-8505-41e8-d7d2-ed1307b2078f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "“…He tested negative for Covid-19 upon his return to Johannesburg (from his trip to West Africa) on December 8, according to his office.” - @CNN\n",
            "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "The End of a Return-to-Office Date https://t.co/QcckO8R0hh via @politicalwire\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "train_vectorizer = CountVectorizer()\n",
        "# d.1. build a dictionary of unique words for training set\n",
        "train_x_bag = train_vectorizer.fit_transform(train_x).todense()\n",
        "test_vectorizer = CountVectorizer(vocabulary=train_vectorizer.get_feature_names())\n",
        "test_x_bag = test_vectorizer.fit_transform(test_x).todense()\n",
        "# d.2. Report feature vectors of 2 reviews\n",
        "print(train_x[10])\n",
        "print(train_x_bag[10])\n",
        "print(train_x[0])\n",
        "print(train_x_bag[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_bag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1H0xsRS4XYm",
        "outputId": "698564fa-c748-4320-81f8-3c05f2f83e93"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [1, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agkBEaOikYaC"
      },
      "source": [
        "## (e) Postprocessing strategy\n",
        "We choose L2 normalization as post-processing method, because:<br/>\n",
        "1) L2 presents the inner product of a vector on itself, representing the length of the vector<br/>\n",
        "2) The similarity between 2 vectors are calculated by their inner product, which is the format of L2<br/>\n",
        "3) So L2 would be an ideal way to constrain the value range of each feature into (0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "collapsed": true,
        "id": "5Eb1oEB_kYaD"
      },
      "outputs": [],
      "source": [
        "# post-processing\n",
        "train_x_bag_normal = normalize(train_x_bag)\n",
        "test_x_bag_normal = normalize(test_x_bag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UACv--x9kYaE"
      },
      "source": [
        "## (f) Sentiment prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "collapsed": true,
        "id": "FLsoSCtlkYaE"
      },
      "outputs": [],
      "source": [
        "def sentiment_prediction(Train_X, Train_Y, Test_X, Test_Y):\n",
        "    # f.1 Logistic regression\n",
        "    lr_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(Train_X, Train_Y)\n",
        "    lr_clf_score = lr_clf.score(Test_X, Test_Y)\n",
        "    print(\"Logistic regression accuracy: {}\".format(lr_clf_score))\n",
        "    \n",
        "    # f.2 Naive Bayes classifier\n",
        "    # Gaussian\n",
        "    gaussian_nb = GaussianNB()\n",
        "    gaussian_nb.fit(Train_X, Train_Y)\n",
        "    gaussian_nb_score = gaussian_nb.score(Test_X, Test_Y)\n",
        "    print(\"Accuracy of Naive Bayes Classifier with Gaussian prior: {}\".format(gaussian_nb_score))\n",
        "\n",
        "    # Bernoulli\n",
        "    b_nb = BernoulliNB()\n",
        "    b_nb.fit(Train_X, Train_Y)\n",
        "    b_nb_score = b_nb.score(Test_X, Test_Y)\n",
        "    print(\"Accuracy of Naive Bayes Classifier with Bernoulli prior: {}\".format(b_nb_score))\n",
        "    \n",
        "    return lr_clf_score, gaussian_nb_score, b_nb_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "0YlUocQJkYaF",
        "outputId": "5ab03091-1e24-4aa8-8a21-28a7ab78cc32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression accuracy: 0.6538461538461539\n",
            "Accuracy of Naive Bayes Classifier with Gaussian prior: 0.6538461538461539\n",
            "Accuracy of Naive Bayes Classifier with Bernoulli prior: 0.5769230769230769\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6538461538461539, 0.6538461538461539, 0.5769230769230769)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "sentiment_prediction(train_x_bag_normal, train_y, test_x_bag_normal, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsP0cDr-kYaH"
      },
      "source": [
        "### Comparison of classifiers:\n",
        "Logistic regression model is slightly better than Naive Bayes classifiers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWPTAXr4kYaH"
      },
      "source": [
        "### Words playing the most important roles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "HOH8LAfckYaI",
        "outputId": "fb0e799c-f073-4d69-93cd-a0519cd70896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top 10 most important words: \n",
            "count(zw) = 744\n",
            "count(zvyvj6dzm5) = 743\n",
            "count(zffwbblmw0) = 742\n",
            "count(zero) = 741\n",
            "count(zaichishka) = 740\n",
            "count(yyesij0cxt) = 739\n",
            "count(your) = 738\n",
            "count(you) = 737\n",
            "count(york) = 736\n",
            "count(yet) = 735\n"
          ]
        }
      ],
      "source": [
        "vocabulary = train_vectorizer.vocabulary_\n",
        "sorted_vocabulary = sorted(vocabulary.items(), key=operator.itemgetter(1), reverse=True)\n",
        "print(\"The top 10 most important words: \")\n",
        "for word in sorted_vocabulary[:10]:\n",
        "    print(\"count({}) = {}\".format(word[0], word[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_D8WFHskYaJ"
      },
      "source": [
        "## (g) N-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "8yU0jhNukYaK",
        "outputId": "1f48d987-596d-47e8-8331-5a9fafe7be6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "“…He tested negative for Covid-19 upon his return to Johannesburg (from his trip to West Africa) on December 8, according to his office.” - @CNN\n",
            "[[0 0 0 ... 0 0 0]]\n",
            "The End of a Return-to-Office Date https://t.co/QcckO8R0hh via @politicalwire\n",
            "[[0 0 0 ... 0 0 0]]\n",
            "Logistic regression accuracy: 0.6923076923076923\n",
            "Accuracy of Naive Bayes Classifier with Gaussian prior: 0.6153846153846154\n",
            "Accuracy of Naive Bayes Classifier with Bernoulli prior: 0.5384615384615384\n",
            "count(zw but) = 1408\n",
            "count(zero front) = 1407\n",
            "count(zaichishka got) = 1406\n",
            "count(yyesij0cxt https) = 1405\n",
            "count(your upcoming) = 1404\n",
            "count(your mailbox) = 1403\n",
            "count(you would) = 1402\n",
            "count(you with) = 1401\n",
            "count(you why) = 1400\n",
            "count(you walk) = 1399\n",
            "The top 10 most important 2-gram words: \n",
            "['zw but', 'zero front', 'zaichishka got', 'yyesij0cxt https', 'your upcoming', 'your mailbox', 'you would', 'you with', 'you why', 'you walk']\n"
          ]
        }
      ],
      "source": [
        "# Vectorize with 2-gram model\n",
        "train_vectorizer_2gram = CountVectorizer(ngram_range=(2, 2))\n",
        "# build a dictionary of unique words for training set\n",
        "train_x_2gram = train_vectorizer_2gram.fit_transform(train_x).todense()\n",
        "test_vectorizer_2gram = CountVectorizer(ngram_range=(2, 2), vocabulary=train_vectorizer_2gram.get_feature_names())\n",
        "test_x_2gram = test_vectorizer_2gram.fit_transform(test_x).todense()\n",
        "# Report feature vectors of 2 reviews\n",
        "print(train_x[10])\n",
        "print(train_x_2gram[10])\n",
        "print(train_x[0])\n",
        "print(train_x_2gram[0])\n",
        "\n",
        "# post-processing\n",
        "train_x_2gram_normal = normalize(train_x_2gram)\n",
        "test_x_2gram_normal = normalize(test_x_2gram)\n",
        "\n",
        "sentiment_prediction(train_x_2gram_normal, train_y, test_x_2gram_normal, test_y)\n",
        "# # Logistic regression\n",
        "# lr_clf_2gram = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(train_x_2gram_normal, train_y)\n",
        "# lr_clf_2gram_score = lr_clf_2gram.score(test_x_2gram_normal, test_y)\n",
        "# print(\"Logistic regression accuracy: {}\".format(lr_clf_2gram_score))\n",
        "\n",
        "# # Naive Bayes classifier\n",
        "# # Gaussian\n",
        "# gaussian_nb_2gram = GaussianNB()\n",
        "# gaussian_nb_2gram.fit(train_x_2gram_normal, train_y)\n",
        "# gaussian_nb_2gram_score = gaussian_nb_2gram.score(test_x_2gram_normal, test_y)\n",
        "# print(\"Accuracy of Naive Bayes Classifier with Gaussian prior: {}\".format(gaussian_nb_2gram_score))\n",
        "\n",
        "# # Bernoulli\n",
        "# b_nb_2gram = BernoulliNB()\n",
        "# b_nb_2gram.fit(train_x_2gram_normal, train_y)\n",
        "# b_nb_2gram_score = b_nb_2gram.score(test_x_2gram_normal, test_y)\n",
        "# print(\"Accuracy of Naive Bayes Classifier with Bernoulli prior: {}\".format(b_nb_2gram_score))\n",
        "\n",
        "# Most important 2-gram words\n",
        "vocabulary_2gram = train_vectorizer_2gram.vocabulary_\n",
        "sorted_vocabulary_2gram = sorted(vocabulary_2gram.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "important_words = []\n",
        "for word in sorted_vocabulary_2gram[:10]:\n",
        "    important_words.append(word[0])\n",
        "    print(\"count({}) = {}\".format(word[0], word[1]))\n",
        "\n",
        "print(\"The top 10 most important 2-gram words: \")\n",
        "print(important_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk7Ip8FwkYaL"
      },
      "source": [
        "## (h) PCA for bag of words model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "sGGN2k_dkYaL"
      },
      "outputs": [],
      "source": [
        "#Use SVD to peform PCA\n",
        "p,n = np.shape(train_x_bag_normal)\n",
        "cov_Mat = np.dot(train_x_bag_normal.T, train_x_bag_normal)/(p-1)\n",
        "u, s, vh = np.linalg.svd(cov_Mat, full_matrices=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "SNiWJzs6kYaL",
        "outputId": "0b808b29-fb32-4372-8046-2baa1d317d83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression accuracy: 0.5769230769230769\n",
            "Accuracy of Naive Bayes Classifier with Gaussian prior: 0.6153846153846154\n",
            "Accuracy of Naive Bayes Classifier with Bernoulli prior: 0.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5769230769230769, 0.6153846153846154, 0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "train_x_10 = np.dot(train_x_bag_normal, u[:,:10])\n",
        "test_x_10 = np.dot(test_x_bag_normal, u[:,:10])\n",
        "sentiment_prediction(train_x_10, train_y, test_x_10, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "NoYLygbkkYaN",
        "outputId": "97b72abb-6f18-48b8-cf95-46c9173f22f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression accuracy: 0.6538461538461539\n",
            "Accuracy of Naive Bayes Classifier with Gaussian prior: 0.6153846153846154\n",
            "Accuracy of Naive Bayes Classifier with Bernoulli prior: 0.6153846153846154\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6538461538461539, 0.6153846153846154, 0.6153846153846154)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "train_x_50 = np.dot(train_x_bag_normal, u[:,:50])\n",
        "test_x_50 = np.dot(test_x_bag_normal, u[:,:50])\n",
        "sentiment_prediction(train_x_50, train_y, test_x_50, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "rU9GWPx3kYaN",
        "outputId": "ef349cc0-ffd0-4db0-b5d8-0951a7e993b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression accuracy: 0.6538461538461539\n",
            "Accuracy of Naive Bayes Classifier with Gaussian prior: 0.5769230769230769\n",
            "Accuracy of Naive Bayes Classifier with Bernoulli prior: 0.5384615384615384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6538461538461539, 0.5769230769230769, 0.5384615384615384)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "train_x_100 = np.dot(train_x_bag_normal, u[:,:100])\n",
        "test_x_100 = np.dot(test_x_bag_normal, u[:,:100])\n",
        "sentiment_prediction(train_x_100, train_y, test_x_100, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "collapsed": true,
        "id": "Jvf_URu_kYaO"
      },
      "outputs": [],
      "source": [
        "def meanX(dataX):\n",
        "    return np.mean(dataX, axis=0)\n",
        "def pca(XMat, k):\n",
        "    average = meanX(XMat) \n",
        "    m, n = np.shape(XMat)\n",
        "    data_adjust = []\n",
        "    avgs = np.tile(average, (m, 1))\n",
        "    data_adjust = XMat - avgs\n",
        "    covX = np.cov(data_adjust.T)\n",
        "    featValue, featVec=  np.linalg.eig(covX)\n",
        "    index = np.argsort(-featValue)\n",
        "    finalData = []\n",
        "    if k > n:\n",
        "        print(\"k must lower than feature number\")\n",
        "        return\n",
        "    else:\n",
        "        selectVec = np.matrix(featVec.T[index[:k]])\n",
        "        finalData = data_adjust * selectVec.T \n",
        "        reconData = (finalData * selectVec) + average  \n",
        "        finalData = finalData.astype('float64')\n",
        "    return finalData, reconData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "fq_Tw0_PkYaO",
        "outputId": "e95f092f-7844-4ce3-a88f-d4d5f5d62d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression accuracy: 0.6538461538461539\n",
            "Accuracy of Naive Bayes Classifier with Gaussian prior: 0.5769230769230769\n",
            "Accuracy of Naive Bayes Classifier with Bernoulli prior: 0.5384615384615384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6538461538461539, 0.5769230769230769, 0.5384615384615384)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "train_x_10, _recon_train = pca(train_x_bag_normal, 10)\n",
        "test_x_10, _recon_test = pca(test_x_bag_normal, 10)\n",
        "sentiment_prediction(train_x_10, train_y, test_x_10, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "y5lyUJnDkYaO",
        "outputId": "3cbfcb83-482f-4877-d2ac-73e66751888e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression accuracy: 0.6923076923076923\n",
            "Accuracy of Naive Bayes Classifier with Gaussian prior: 0.5\n",
            "Accuracy of Naive Bayes Classifier with Bernoulli prior: 0.46153846153846156\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6923076923076923, 0.5, 0.46153846153846156)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "# PCA with 50 components\n",
        "\n",
        "train_x_50, _recon_train = pca(train_x_bag_normal, 50)\n",
        "test_x_50, _recon_test = pca(test_x_bag_normal, 50)\n",
        "sentiment_prediction(train_x_50, train_y, test_x_50, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "eACV_mt9kYaO",
        "outputId": "2f9f72fb-09f7-436d-d14e-8fb6a78ee5f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression accuracy: 0.6923076923076923\n",
            "Accuracy of Naive Bayes Classifier with Gaussian prior: 0.5\n",
            "Accuracy of Naive Bayes Classifier with Bernoulli prior: 0.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6923076923076923, 0.5, 0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "# PCA with 100 components\n",
        "# pca_100 = PCA(n_components=100)\n",
        "\n",
        "train_x_100, _recon_train = pca(train_x_bag_normal, 100)\n",
        "test_x_100, _recon_test = pca(test_x_bag_normal, 100)\n",
        "sentiment_prediction(train_x_100, train_y, test_x_100, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3kK88g_kYaP"
      },
      "source": [
        "----------------------------"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}